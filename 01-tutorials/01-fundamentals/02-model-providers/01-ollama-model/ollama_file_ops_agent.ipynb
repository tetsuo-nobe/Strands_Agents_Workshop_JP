{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strands Agents と Ollama モデルを使用したローカルエージェントの構築\n",
    "\n",
    "このノートブックでは、Strands Agent と Ollama を使用してパーソナルエージェントを作成する方法を説明します。このエージェントは、ファイル操作、Web 検索、システムコマンドなど、さまざまなローカルタスクを実行できます。\n",
    "\n",
    "## Ollama とは？\n",
    "\n",
    "[Ollama](https://ollama.com/) は、大規模言語モデル (LLM) をローカルマシン上で実行できるオープンソースフレームワークです。これらのモデルを操作するためのシンプルな API を提供しているため、外部サービスにデータを送信したくないプライバシー重視のアプリケーションに最適です。\n",
    "\n",
    "Ollama の主なメリット:\n",
    "- **プライバシー**: すべての処理がローカルマシン上で実行されます\n",
    "- **API コストなし**: 好きなだけ無料で使用できます\n",
    "- **オフライン機能**: インターネット接続なしで動作します\n",
    "- **カスタマイズ**: 特定の用途に合わせて微調整できます\n",
    "\n",
    "## エージェントの詳細\n",
    "\n",
    "<div style=\"float: left; margin-right: 20px;\">\n",
    "    \n",
    "|機能                |説明                                        |\n",
    "|--------------------|---------------------------------------------------|\n",
    "|基盤モデル           |Ollama mモデル - ファイル操作エージェントの作成       |\n",
    "|エージェントの構造    |シングルエージェントアーキテクチャ                    |\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェントアーキテクチャ\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"images/architecture.png\" width=\"65%\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップとインストール\n",
    "\n",
    "このノートブックを実行する前に、以下の準備が整っていることを確認してください。\n",
    "\n",
    "1. Ollama をインストールします。[https://ollama.com/download](https://ollama.com/download) の手順に従ってください。\n",
    "2. Ollama サーバーを起動します。`ollama serve`\n",
    "3. Ollama を使用してモデルをダウンロードします。`ollama pull llama3.2:1b`\n",
    "\n",
    "詳細な手順については、[ドキュメント](https://cuddly-sniffle-lrmk2y7.pages.github.io/0.1.x/user-guide/concepts/model-providers/ollama/) を参照してください。\n",
    "\n",
    "このノートブックでは、SageMaker Studio との互換性を確保するために、Linux ディストリビューション用の Ollama をダウンロードします。これは、Workshop Studio での AWS 主導ワークショップ中にコードを実行するために行われます。このコードをローカルで実行する場合は、現在の環境に Ollama をダウンロードする手順を調整する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # これは Linux コンピュータで動作します\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.Popen(['ollama', 'serve'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "# Strands コンポーネントのインポート\n",
    "from strands import Agent, tool\n",
    "from strands.models.ollama import OllamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama の実行確認\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    print(\"✅ Ollama is running. Available models:\")\n",
    "    for model in response.json().get(\"models\", []):\n",
    "        print(f\"- {model['name']}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"❌ Ollama is not running. Please start Ollama before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カスタムツールの定義\n",
    "\n",
    "ツールとは、エージェントが環境とインタラクトするために使用できる関数です。以下では、パーソナルエージェントに役立つツールをいくつか定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイル操作ツール\n",
    "\n",
    "\n",
    "@tool\n",
    "def file_read(file_path: str) -> str:\n",
    "    \"\"\"Read a file and return its content.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file to read\n",
    "\n",
    "    Returns:\n",
    "        str: Content of the file\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file doesn't exist\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File '{file_path}' not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error reading file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def file_write(file_path: str, content: str) -> str:\n",
    "    \"\"\"Write content to a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file\n",
    "        content (str): The content to write to the file\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating success or failure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ディレクトリが存在しない場合は作成する\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(file_path)), exist_ok=True)\n",
    "\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(content)\n",
    "        return f\"File '{file_path}' written successfully.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing to file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_directory(directory_path: str = \".\") -> str:\n",
    "    \"\"\"List files and directories in the specified path.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory to list\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string listing all files and directories\n",
    "    \"\"\"\n",
    "    try:\n",
    "        items = os.listdir(directory_path)\n",
    "        files = []\n",
    "        directories = []\n",
    "\n",
    "        for item in items:\n",
    "            full_path = os.path.join(directory_path, item)\n",
    "            if os.path.isdir(full_path):\n",
    "                directories.append(f\"Folder: {item}/\")\n",
    "            else:\n",
    "                files.append(f\"File: {item}\")\n",
    "\n",
    "        result = f\"Contents of {os.path.abspath(directory_path)}:\\n\"\n",
    "        result += (\n",
    "            \"\\nDirectories:\\n\" + \"\\n\".join(sorted(directories))\n",
    "            if directories\n",
    "            else \"\\nNo directories found.\"\n",
    "        )\n",
    "        result += (\n",
    "            \"\\n\\nFiles:\\n\" + \"\\n\".join(sorted(files)) if files else \"\\nNo files found.\"\n",
    "        )\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error listing directory: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama を利用したエージェントの作成\n",
    "\n",
    "Ollama モデルと上記で定義したツールを使ってエージェントを作成します。\n",
    "\n",
    "注: `execute_commands`、`search_files` などのツールを追加することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントのための包括的なシステムプロンプトの定義\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful personal assistant capable of performing local file actions and simple tasks for the user.\n",
    "\n",
    "Your key capabilities:\n",
    "1. Read, understand, and summarize files.\n",
    "2. Create and write to files.\n",
    "3. List directory contents and provide information on the files.\n",
    "4. Summarize text content\n",
    "\n",
    "When using tools:\n",
    "- Always verify file paths before operations\n",
    "- Be careful with system commands\n",
    "- Provide clear explanations of what you're doing\n",
    "- If a task cannot be completed, explain why and suggest alternatives\n",
    "\n",
    "Always be helpful, concise, and focus on addressing the user's needs efficiently.\n",
    "\"\"\"\n",
    "\n",
    "model_id = (\n",
    "    \"llama3.2:1b\"  # Ollama で取得した任意のモデルに変更可能\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama モデルを設定する\n",
    "Ollama サービスが http://localhost:11434 で実行されていること、および `model_id` が上記に表示された Ollama モデルのリストに含まれていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = OllamaModel(\n",
    "    model_id=model_id,\n",
    "    host=\"http://localhost:11434\",\n",
    "    params={\n",
    "        \"max_tokens\": 4096,  # モデルの能力に応じて調整\n",
    "        \"temperature\": 0.7,  # 低いほど決定論的な回答、高いほど創造的な回答\n",
    "        \"top_p\": 0.9,  # 核サンプリングパラメータ\n",
    "        \"stream\": True,  # ストリーミング応答の有効化\n",
    "    },\n",
    ")\n",
    "\n",
    "# エージェントの作成\n",
    "local_agent = Agent(\n",
    "    system_prompt=system_prompt,\n",
    "    model=ollama_model,\n",
    "    tools=[file_read, file_write, list_directory],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エージェントのテスト\n",
    "\n",
    "いくつかのサンプルタスクを使ってエージェントをテストしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_agent(\n",
    "    \"Read the file in the path: sample_file/Amazon-com-Inc-2023-Shareholder-Letter-jp.txt and summarize it in 5 bullet points in Japanese.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例 2: 現在のディレクトリ内のファイルの一覧表示\n",
    "response = local_agent(\"Show me the files in the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例 3: サンプルファイルの作成\n",
    "response = local_agent(\n",
    "    \"Create a file called 'sample.txt' with the content 'This is a test file created by my Ollama agent.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例 4: 複数のファイルを読んで理解した後に README ファイルを作成する\n",
    "response = local_agent(\"Create a readme.md for the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、Stands と Ollama を使用してローカルパーソナルエージェントを作成しました。エージェントはファイル操作（読み取り、書き込み、追加）とテキストの要約/分析を実行できます。\n",
    "\n",
    "これは、Ollama を用いて AI モデルをローカルで実行することの威力と、Strands のツールシステムの柔軟性を組み合わせたものです。ニーズに応じてツールを追加したり、異なる Ollama モデルを使用したりすることで、このエージェントを拡張できます。\n",
    "\n",
    "### 次のステップ（アイデア）\n",
    "\n",
    "- さまざまな Ollama モデルを試して、エージェントの機能にどのような影響があるかを確認する\n",
    "- Web 検索、メール送信、カレンダー統合などのより複雑なツールを追加する\n",
    "- エージェントが過去のインタラクションを記憶するためのメモリを実装する\n",
    "- エージェントと対話するためのシンプルな UI を作成する"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
